---
# Dataset specifications --------------------------------------

out_dir: "./out" # Location where data will be stored
pretrain_data: "./data/uniref_example.tar.gz" # Name of the dataset file for pretraining in compressed tsv format
pretrain_file_format: {"sequence": 1} # "EC(-.-.-)": 0
train_data: "./data/mdh_train_subset.tar.gz" # Name of the train dataset file for fine-tuning in compressed tsv format
train_file_format: {"sequence": 2} # "upsampling": 0, "EC(-.-.-)": 1
val_data: "./data/mdh_val_subset.tar.gz" # Name of the validation dataset file for fine-tuning in compressed tsv format
weights: "path/to/pretrained_weigths_if_any.ckpt" # Weights path for finetuning
wandb_project: project_name # Project name for Wandb logging
wandb_name: experiment_name # Name fo the experiment for Wandb logging

# Architecture details --------------------------------------

devices: 4 # Number of devices
num_workers: 28 # Number of workers

# Training hyperparams --------------------------------------

pretrain: True # "True" if pretraining, "False" if fine-tuning
epochs: 1000 # Number of steps to train
batch_size: 256 # Number of elements in input batch
lr: 0.00001 # Learning rate
lr_step_size: 200 # Period of learning rate decay
gamma: 0.75 #  Multiplicative factor of learning rate decay
p_masking: 0.1 # Cloze residue masking rate
